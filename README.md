# iGniter
iGniter, an interference-aware GPU resource provisioning framework for achieving predictable performance of DNN inference in the cloud. 

## Prototype of iGniter

Our iGniter framework comprises three pieces of modules: an inference workload placer and a GPU resource allocator as well as an inference performance predictor. With the profiled model coefficients, the inference performance predictor first estimates the inference latency using our performance model. It then guides our GPU resource allocator and inference workload placer to identify an appropriate GPU device with the least performance interference and the guaranteed SLOs from candidate GPUs for each inference workload. According to the cost-efficient GPU resource provisioning plan generated by our algorithm, the GPU device launcher finally builds a GPU cluster and launches the Triton inference serving process for each DNN inference workload on the provisioned GPU devices.

![](https://github.com/icloud-ecnu/igniter/blob/main/images/prototype.png)

## Running

### Obtaining the GPU resources provisioning plan

```
$ cd i-Gniter/Algorithm
$ python3 ./igniter-algorithm.py
```

### Download Docker Image From NGC
Before you can use the Triton Docker image you must install Docker. If you plan on using a GPU for inference you must also install the NVIDIA Container Toolkit.
```
$ docker pull nvcr.io/nvidia/tritonserver:21.07-py3
$ docker pull nvcr.io/nvidia/tritonserver:21.07-py3-sdk
```

### Process the data into json format

If you want to use real data for inference,
```
$ cd i-Gniter/Launch
$ python3 ./data_transfer.py -c 1000 -d /your/pictures/abspath -j ./input_data -f your_file_name.json
```

### Modify the config.json file for inference
There is a config file for example:
```
{
    "models": ["alexnet_dynamic","resnet50_dynamic","ssd_dynamic"],
    "rates": [1200,600,50],
    "slos": [5,15,20],
    "resources": [30,45,15],
    "batches": [6,9,1]
}
```

### Measure the performance

```
$ python3 ./evaluation.py -c ./config.json -t 10 -s 10
```
